# â“ ÄŒasto kladenÃ© otÃ¡zky (FAQ)

## ğŸ“ Pro studenty

### ZÃ¡kladnÃ­ otÃ¡zky

**Q: Co je to vlastnÄ› embedding?**
> A: Embedding je zpÅ¯sob, jak reprezentovat slova pomocÃ­ ÄÃ­sel. PÅ™edstavte si to jako "souÅ™adnice" slova v mnohorozmÄ›rnÃ©m prostoru. PodobnÃ¡ slova majÃ­ podobnÃ© souÅ™adnice.

**Q: ProÄ program zobrazuje VÅ ECH 768 ÄÃ­sel? To je pÅ™ece neÄitelnÃ©!**
> A: PrÃ¡vÄ› to je pointa! Chceme, abyste vidÄ›li:
> - Jak obrovskÃ© mnoÅ¾stvÃ­ informacÃ­ model pouÅ¾Ã­vÃ¡ pro jedinÃ© slovo
> - Å½e neexistuje "jedno ÄÃ­slo = jeden vÃ½znam"  
> - SloÅ¾itost toho, jak AI "myslÃ­"
> - Å½e lidskÃ½ mozek nedokÃ¡Å¾e tato ÄÃ­sla interpretovat pÅ™Ã­mo

**Q: ProÄ 768 dimenzÃ­? ProÄ ne jen 3 jako v bÄ›Å¾nÃ©m prostoru?**
> A: Jazyk je sloÅ¾itÃ½! PotÅ™ebujeme mnoho dimenzÃ­, abychom zachytili vÅ¡echny nuance vÃ½znamu. 768 je kompromis mezi vÃ½konem a pÅ™esnostÃ­. VÄ›tÅ¡Ã­ modely (GPT-3) pouÅ¾Ã­vajÃ­ jeÅ¡tÄ› vÃ­ce dimenzÃ­.

**Q: Co znamenÃ¡ ten divnÃ½ symbol Ä ?**
> A: Ä  znamenÃ¡, Å¾e pÅ™ed tokenem je mezera. Je to zpÅ¯sob, jak model rozliÅ¡uje "Hello" na zaÄÃ¡tku vÄ›ty od " Hello" uprostÅ™ed vÄ›ty.

### TechnickÃ© otÃ¡zky

**Q: MÅ¯Å¾u z embeddingu zÃ­skat zpÄ›t pÅ¯vodnÃ­ slovo?**
> A: Ne pÅ™Ã­mo. Embedding â†’ slovo nenÃ­ jednoznaÄnÃ©. Model pÅ™i generovÃ¡nÃ­ textu pouÅ¾Ã­vÃ¡ embeddingy k vÃ½poÄtu pravdÄ›podobnostÃ­ vÅ¡ech moÅ¾nÃ½ch slov.

**Q: Jak model vÃ­, Å¾e "bank" mÅ¯Å¾e znamenat "banka" i "bÅ™eh"?**
> A: GPT-2 to nevÃ­! PouÅ¾Ã­vÃ¡ stejnÃ½ embedding. NovÄ›jÅ¡Ã­ modely (BERT, GPT-3) pouÅ¾Ã­vajÃ­ kontextovÃ© embeddingy, kterÃ© se mÄ›nÃ­ podle okolnÃ­ch slov.

**Q: ProÄ nÄ›kterÃ¡ slova majÃ­ zÃ¡pornÃ© hodnoty v embeddingu?**
> A: ÄŒÃ­sla v embeddingu nemajÃ­ pÅ™Ã­mÃ½ vÃ½znam. DÅ¯leÅ¾itÃ© jsou vztahy mezi nimi. ZÃ¡pornÃ© hodnoty jsou normÃ¡lnÃ­ - jde o relativnÃ­ pozice v prostoru.

### PraktickÃ© otÃ¡zky

**Q: ProÄ prvnÃ­ spuÅ¡tÄ›nÃ­ trvÃ¡ tak dlouho?**
> A: Stahuje se model (~500 MB). PÅ™i dalÅ¡Ã­ch spuÅ¡tÄ›nÃ­ch uÅ¾ bude rychlÃ©.

**Q: MÅ¯Å¾u pouÅ¾Ã­t jinÃ½ model neÅ¾ GPT-2?**
> A: Ano! Zkuste zmÄ›nit `model_name = "gpt2"` na `"gpt2-medium"` nebo `"gpt2-large"` (pozor, jsou vÄ›tÅ¡Ã­).

**Q: Jak vizualizovat embeddingy?**
> A: Embeddingy majÃ­ 768 dimenzÃ­, coÅ¾ nejde zobrazit. MÅ¯Å¾ete pouÅ¾Ã­t PCA nebo t-SNE k redukci na 2D/3D. Viz bonusovÃ© Ãºkoly.

### KonceptuÃ¡lnÃ­ otÃ¡zky

**Q: Je embedding slova vÅ¾dy stejnÃ½?**
> A: V GPT-2 ano. V kontextovÃ½ch modelech (BERT) ne - mÄ›nÃ­ se podle vÄ›ty.

**Q: Jak se model nauÄil tyto embeddingy?**
> A: TrÃ©novÃ¡nÃ­m na miliardÃ¡ch slov textu. Model se uÄil pÅ™edpovÃ­dat dalÅ¡Ã­ slovo a pÅ™itom si vytvoÅ™il uÅ¾iteÄnÃ© reprezentace.

**Q: MÅ¯Å¾u vytvoÅ™it vlastnÃ­ embeddingy?**
> A: Ano! MÅ¯Å¾ete model dotrÃ©novat (fine-tuning) na vlastnÃ­ch datech. To je ale pokroÄilÃ© tÃ©ma.

### ÄŒastÃ¡ nedorozumÄ›nÃ­

**âŒ MÃ½tus:** "KaÅ¾dÃ© ÄÃ­slo v embeddingu mÃ¡ konkrÃ©tnÃ­ vÃ½znam"
> âœ… **Realita:** VÃ½znam vznikÃ¡ aÅ¾ kombinacÃ­ vÅ¡ech ÄÃ­sel dohromady.

**âŒ MÃ½tus:** "Embeddingy jsou jako slovnÃ­k"
> âœ… **Realita:** Embeddingy zachycujÃ­ podobnosti a vztahy, ne definice.

**âŒ MÃ½tus:** "Model chÃ¡pe, co slova znamenajÃ­"
> âœ… **Realita:** Model mÃ¡ statistickÃ© asociace, ne lidskÃ© porozumÄ›nÃ­.

### Troubleshooting

**Chyba: "CUDA out of memory"**
> Å˜eÅ¡enÃ­: Model bÄ›Å¾Ã­ na CPU, pokud nemÃ¡te GPU. To je v poÅ™Ã¡dku, jen pomalejÅ¡Ã­.

**Chyba: "Token indices sequence length is longer"**
> Å˜eÅ¡enÃ­: VÃ¡Å¡ text je moc dlouhÃ½. GPT-2 zvlÃ¡dne max 1024 tokenÅ¯.

**Chyba: "ModuleNotFoundError"**
> Å˜eÅ¡enÃ­: Nainstalujte chybÄ›jÃ­cÃ­ knihovnu: `pip install transformers torch`

### ZajÃ­mavosti

**ğŸ¤” VÄ›dÄ›li jste, Å¾e...**
- GPT-2 byl povaÅ¾ovÃ¡n za "nebezpeÄnÃ½" a OpenAI ho nejdÅ™Ã­v nezveÅ™ejnila?
- NejvÄ›tÅ¡Ã­ GPT-2 mÃ¡ 1.5 miliardy parametrÅ¯?
- Tokenizer GPT-2 umÃ­ 50,257 rÅ¯znÃ½ch tokenÅ¯?
- NÄ›kterÃ© jazyky (ÄÃ­nÅ¡tina) potÅ™ebujÃ­ mnohem vÃ­ce tokenÅ¯?

### Kam dÃ¡l?

**Chci se dozvÄ›dÄ›t vÃ­ce o:**
- **Transformerech** â†’ "Attention Is All You Need" paper
- **BERT** â†’ KontextovÃ© embeddingy
- **GPT-3/4** â†’ VelkÃ© jazykovÃ© modely
- **Fine-tuning** â†’ Jak pÅ™izpÅ¯sobit model vlastnÃ­m datÅ¯m
- **Prompt engineering** â†’ Jak efektivnÄ› komunikovat s LLM

### OtÃ¡zky pro pokroÄilÃ©

**Q: Jak funguje attention mechanismus?**
> A: To je nad rÃ¡mec tohoto projektu, ale zjednoduÅ¡enÄ›: model se "dÃ­vÃ¡" na rÅ¯znÃ¡ slova s rÅ¯znou "pozornostÃ­" pÅ™i vytvÃ¡Å™enÃ­ embeddingÅ¯.

**Q: JakÃ½ je rozdÃ­l mezi GPT a BERT?**
> A: GPT Äte zleva doprava, BERT Äte obousmÄ›rnÄ›. GPT je lepÅ¡Ã­ na generovÃ¡nÃ­, BERT na porozumÄ›nÃ­.

**Q: MÅ¯Å¾u embeddingy pouÅ¾Ã­t pro vlastnÃ­ projekt?**
> A: Ano! Embeddingy jsou skvÄ›lÃ© pro:
> - VyhledÃ¡vÃ¡nÃ­ podobnÃ½ch textÅ¯
> - Klasifikaci sentimentu  
> - Clustering dokumentÅ¯
> - DoporuÄovacÃ­ systÃ©my

---

ğŸ’¡ **MÃ¡te dalÅ¡Ã­ otÃ¡zku?** Zeptejte se vyuÄujÃ­cÃ­ho nebo vytvoÅ™te issue na GitHubu!
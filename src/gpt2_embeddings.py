"""
üéì GPT-2 Embeddings Explorer - Vzdƒõl√°vac√≠ n√°stroj

Tento program pom√°h√° student≈Øm pochopit:
1. Co jsou embeddingy - jak poƒç√≠taƒç "ch√°pe" slova pomoc√≠ ƒç√≠sel
2. Jak funguje tokenizace - rozdƒõlen√≠ textu na men≈°√≠ ƒç√°sti
3. Jak model v√≠ o po≈ôad√≠ slov - poziƒçn√≠ embeddingy
4. Jak jazykov√© modely internƒõ reprezentuj√≠ text

Autor: jirpo9
√öƒçel: V√Ωuka z√°klad≈Ø NLP a jazykov√Ωch model≈Ø
"""

import math
from typing import List, Tuple, Optional
import torch
from transformers import GPT2Tokenizer, GPT2Model


class GPT2EmbeddingsExplorer:
    """T≈ô√≠da pro exploraci embedding≈Ø v GPT-2 modelu."""
    
    def __init__(self, model_name: str = "gpt2"):
        """
        Inicializace exploreru.
        
        Args:
            model_name: N√°zev modelu z Hugging Face (default: "gpt2")
        """
        try:
            self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)
            self.model = GPT2Model.from_pretrained(model_name)
            self.model.eval()
            self.model_name = model_name
            self.embedding_dim = self.model.config.hidden_size
            self.max_length = self.model.config.max_position_embeddings
            self.vocab_size = len(self.tokenizer.get_vocab())
        except Exception as e:
            raise RuntimeError(f"Chyba p≈ôi naƒç√≠t√°n√≠ modelu: {e}")
    
    def get_model_info(self) -> dict:
        """Vr√°t√≠ informace o modelu."""
        return {
            "model_name": self.model_name,
            "vocab_size": self.vocab_size,
            "embedding_dim": self.embedding_dim,
            "max_sequence_length": self.max_length
        }
    
    def tokenize_text(self, text: str) -> Tuple[torch.Tensor, List[str]]:
        """
        Tokenizuje vstupn√≠ text.
        
        Args:
            text: Vstupn√≠ text
            
        Returns:
            Tuple obsahuj√≠c√≠ token IDs a seznam token≈Ø
        """
        if len(text.split()) > self.max_length:
            print(f"Varov√°n√≠: Text bude o≈ô√≠znut na {self.max_length} token≈Ø.")
        
        inputs = self.tokenizer(text, return_tensors="pt", 
                               truncation=True, max_length=self.max_length)
        tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
        
        return inputs, tokens
    
    def get_token_embeddings(self, inputs: dict) -> torch.Tensor:
        """
        Z√≠sk√° tokenov√© embeddingy z modelu.
        
        Args:
            inputs: Tokenizovan√Ω vstup
            
        Returns:
            Tensor s embeddingy
        """
        with torch.no_grad():
            outputs = self.model(**inputs)
        return outputs.last_hidden_state
    
    def get_learned_positional_embeddings(self, length: int) -> torch.Tensor:
        """
        Z√≠sk√° nauƒçen√© poziƒçn√≠ embeddingy z GPT-2.
        
        Args:
            length: D√©lka sekvence
            
        Returns:
            Tensor s poziƒçn√≠mi embeddingy
        """
        position_ids = torch.arange(length).unsqueeze(0)
        return self.model.wpe(position_ids)
    
    def compute_sinusoidal_positional_embedding(self, position: int, 
                                                dim: int, max_length: int = 10000) -> torch.Tensor:
        """
        Vypoƒç√≠t√° sinusoid√°ln√≠ poziƒçn√≠ embedding (pro srovn√°n√≠).
        
        Args:
            position: Pozice v sekvenci
            dim: Dimenze embeddingu
            max_length: Maxim√°ln√≠ d√©lka pro ≈°k√°lov√°n√≠
            
        Returns:
            Poziƒçn√≠ embedding
        """
        embedding = torch.zeros(dim)
        for i in range(0, dim, 2):
            if i + 1 < dim:
                embedding[i] = math.sin(position / (max_length ** (i / dim)))
                embedding[i + 1] = math.cos(position / (max_length ** ((i + 1) / dim)))
        return embedding
    
    def analyze_text(self, text: str, show_details: bool = True) -> dict:
        """
        Analyzuje embeddingy pro zadan√Ω text.
        
        Args:
            text: Vstupn√≠ text
            show_details: Zda zobrazit detaily pro ka≈æd√Ω token
            
        Returns:
            Slovn√≠k s v√Ωsledky anal√Ωzy
        """
        # Tokenizace
        inputs, tokens = self.tokenize_text(text)
        
        # Z√≠sk√°n√≠ embedding≈Ø
        token_embeddings = self.get_token_embeddings(inputs)
        learned_pos_embeddings = self.get_learned_positional_embeddings(len(tokens))
        
        results = {
            "tokens": tokens,
            "token_count": len(tokens),
            "token_embeddings": [],
            "positional_embeddings": [],
            "combined_embeddings": [],
            "average_embedding": None
        }
        
        if show_details:
            print(f"\nTokeny v zadan√©m textu: {tokens}")
            print("\nAnal√Ωza jednotliv√Ωch token≈Ø:")
            print("-" * 50)
        
        all_combined_embeddings = []
        
        for idx, token in enumerate(tokens):
            # Tokenov√Ω embedding
            token_emb = token_embeddings[0, idx]
            
            # Poziƒçn√≠ embeddingy (nauƒçen√© z GPT-2)
            pos_emb = learned_pos_embeddings[0, idx]
            
            # Kombinovan√© embeddingy (jak je GPT-2 skuteƒçnƒõ pou≈æ√≠v√°)
            # GPT-2 sƒç√≠t√° tokenov√© a poziƒçn√≠ embeddingy
            combined_emb = token_emb  # V GPT-2 u≈æ jsou poziƒçn√≠ embeddingy zahrnuty
            
            results["token_embeddings"].append(token_emb)
            results["positional_embeddings"].append(pos_emb)
            results["combined_embeddings"].append(combined_emb)
            all_combined_embeddings.append(combined_emb)
            
            if show_details:
                print(f"\nToken #{idx}: '{token}'")
                print(f"  Norma tokenov√©ho emb.: {token_emb.norm().item():.4f}")
                print(f"  Norma poziƒçn√≠ho emb.: {pos_emb.norm().item():.4f}")
                
                # VZDƒöL√ÅVAC√ç √öƒåEL: Zobraz√≠me V≈†ECH 768 hodnot, aby studenti vidƒõli slo≈æitost
                print(f"\n  üìä KOMPLETN√ç TOKENOV√ù EMBEDDING ({len(token_emb)} hodnot):")
                print("  " + "-" * 70)
                # Form√°tovan√© zobrazen√≠ po 8 hodnot√°ch na ≈ô√°dek
                for i in range(0, len(token_emb), 8):
                    values = token_emb[i:i+8].tolist()
                    formatted = [f"{v:7.4f}" for v in values]
                    print(f"  [{i:3d}-{min(i+7, len(token_emb)-1):3d}]: " + " ".join(formatted))
                
                print(f"\n  üìä KOMPLETN√ç POZIƒåN√ç EMBEDDING ({len(pos_emb)} hodnot):")
                print("  " + "-" * 70)
                for i in range(0, len(pos_emb), 8):
                    values = pos_emb[i:i+8].tolist()
                    formatted = [f"{v:7.4f}" for v in values]
                    print(f"  [{i:3d}-{min(i+7, len(pos_emb)-1):3d}]: " + " ".join(formatted))
        
        # V√Ωpoƒçet pr≈Ømƒõrn√©ho embeddingu (bƒõ≈ænƒõj≈°√≠ ne≈æ souƒçet)
        if all_combined_embeddings:
            results["average_embedding"] = torch.stack(all_combined_embeddings).mean(dim=0)
            
            if show_details:
                print("\n" + "=" * 50)
                print("SOUHRN:")
                print(f"Celkov√Ω poƒçet token≈Ø: {len(tokens)}")
                print(f"Dimenze embedding≈Ø: {self.embedding_dim}")
                print(f"\nüéì D≈ÆLE≈ΩIT√â PRO STUDENTY:")
                print("Ka≈æd√Ω token je reprezentov√°n 768 ƒç√≠sly!")
                print("Tato ƒç√≠sla nemaj√≠ jednotliv√Ω v√Ωznam - v√Ωznam vznik√° jejich kombinac√≠.")
                print("\nPr≈Ømƒõrn√Ω embedding cel√©ho textu (v≈°ech 768 hodnot):")
                print("-" * 70)
                
                # Zobrazit V≈†ECHNY hodnoty pr≈Ømƒõrn√©ho embeddingu
                avg_emb = results['average_embedding']
                for i in range(0, len(avg_emb), 8):
                    values = avg_emb[i:i+8].tolist()
                    formatted = [f"{v:7.4f}" for v in values]
                    print(f"[{i:3d}-{min(i+7, len(avg_emb)-1):3d}]: " + " ".join(formatted))
                
                print(f"\nNorma pr≈Ømƒõrn√©ho embeddingu: {avg_emb.norm().item():.4f}")
                print("\nüí° Co vid√≠te?")
                print("- Hodnoty jsou mezi cca -3 a +3")
                print("- Nƒõkter√© jsou bl√≠zko 0, jin√© ne")
                print("- ≈Ω√°dn√° hodnota nem√° 'speci√°ln√≠ v√Ωznam'")
                print("- Model pou≈æ√≠v√° V≈†ECH 768 hodnot pro reprezentaci v√Ωznamu!")
        
        return results


def main():
    """Hlavn√≠ funkce pro interaktivn√≠ pou≈æit√≠."""
    print("GPT-2 Embeddings Explorer")
    print("=" * 50)
    print("\n‚ö†Ô∏è  POZN√ÅMKA: Program zobrazuje V≈†ECH 768 hodnot embedding≈Ø!")
    print("   C√≠lem je uk√°zat skuteƒçnou slo≈æitost reprezentace textu v AI.")
    print("   P≈ôipravte se na HODNƒö ƒç√≠sel! üî¢\n")
    
    # Inicializace
    try:
        explorer = GPT2EmbeddingsExplorer()
        info = explorer.get_model_info()
        print(f"\nNaƒçten model: {info['model_name']}")
        print(f"Velikost slovn√≠ku: {info['vocab_size']:,}")
        print(f"Dimenze embedding≈Ø: {info['embedding_dim']}")
        print(f"Max. d√©lka sekvence: {info['max_sequence_length']}")
    except Exception as e:
        print(f"Chyba p≈ôi inicializaci: {e}")
        return
    
    # Interaktivn√≠ smyƒçka
    while True:
        print("\n" + "-" * 50)
        user_input = input("\nZadejte text k anal√Ωze (nebo 'q' pro ukonƒçen√≠): ")
        
        if user_input.lower() == 'q':
            print("Ukonƒçuji program.")
            break
        
        if not user_input.strip():
            print("Pr√°zdn√Ω vstup, zkuste znovu.")
            continue
        
        try:
            explorer.analyze_text(user_input, show_details=True)
        except Exception as e:
            print(f"Chyba p≈ôi anal√Ωze: {e}")


if __name__ == "__main__":
    main()
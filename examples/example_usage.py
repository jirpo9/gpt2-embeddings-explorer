"""
ğŸ“ VzdÄ›lÃ¡vacÃ­ pÅ™Ã­klady pro pochopenÃ­ embeddingÅ¯ v GPT-2

Tyto pÅ™Ã­klady jsou navrÅ¾eny tak, aby studentÅ¯m pomohly pochopit:
- Jak fungujÃ­ embeddingy
- Co je tokenizace  
- Jak model "vidÃ­" text
- ProÄ jsou nÄ›kterÃ¡ slova podobnÃ¡
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.gpt2_embeddings import GPT2EmbeddingsExplorer
import torch


def ukazka_1_tokenizace():
    """
    UKÃZKA 1: PochopenÃ­ tokenizace
    
    Studenti uvidÃ­:
    - Jak se text rozdÄ›luje na tokeny
    - Å½e tokeny nejsou vÅ¾dy celÃ¡ slova
    - Jak rÅ¯znÃ© jazyky vyÅ¾adujÃ­ rÅ¯znÃ½ poÄet tokenÅ¯
    """
    print("\n" + "="*60)
    print("ğŸ“ UKÃZKA 1: JAK FUNGUJE TOKENIZACE")
    print("="*60)
    print("\nTokenizace = rozdÄ›lenÃ­ textu na menÅ¡Ã­ ÄÃ¡sti (tokeny)")
    print("GPT-2 nevidÃ­ pÃ­smena ani slova, ale TOKENY!\n")
    
    explorer = GPT2EmbeddingsExplorer()
    
    # RÅ¯znÃ© pÅ™Ã­klady pro demonstraci
    texty = [
        "Hello",
        "Hello world",  
        "Hello, world!",
        "Ahoj svÄ›te",  # ÄŒeÅ¡tina
        "KÃ¼nstliche Intelligenz",  # NÄ›mÄina
        "ğŸ‘‹ğŸŒ",  # Emoji
        "https://www.example.com",  # URL
        "3.14159265359"  # ÄŒÃ­sla
    ]
    
    print("PodÃ­vejme se, jak model rozdÄ›luje rÅ¯znÃ© texty:\n")
    
    for text in texty:
        _, tokens = explorer.tokenize_text(text)
        print(f"Text: '{text}'")
        print(f"  â†’ Tokeny: {tokens}")
        print(f"  â†’ PoÄet tokenÅ¯: {len(tokens)}")
        print()
    
    print("ğŸ’¡ CO SI VÅ IMNOUT:")
    print("- Mezera je souÄÃ¡stÃ­ tokenu (Ä  = mezera pÅ™ed slovem)")
    print("- ÄŒeÅ¡tina potÅ™ebuje vÃ­ce tokenÅ¯ (model trÃ©novÃ¡n hlavnÄ› na angliÄtinÄ›)")
    print("- Interpunkce mÅ¯Å¾e bÃ½t samostatnÃ½ token")
    print("- URL a ÄÃ­sla se Äasto dÄ›lÃ­ na mnoho tokenÅ¯")


def ukazka_2_embeddingy():
    """
    UKÃZKA 2: Co jsou embeddingy
    
    Studenti pochopÃ­:
    - Å½e kaÅ¾dÃ½ token mÃ¡ svÅ¯j vektor (seznam ÄÃ­sel)
    - Å½e podobnÃ¡ slova majÃ­ podobnÃ© vektory
    - Jak velkÃ© jsou tyto vektory
    """
    print("\n" + "="*60)
    print("ğŸ“ UKÃZKA 2: CO JSOU EMBEDDINGY")
    print("="*60)
    print("\nEmbedding = reprezentace slova pomocÃ­ ÄÃ­sel (vektor)")
    print("GPT-2 pouÅ¾Ã­vÃ¡ 768 ÄÃ­sel pro kaÅ¾dÃ© slovo!\n")
    
    explorer = GPT2EmbeddingsExplorer()
    
    # Analyzujeme jedno slovo
    word = "cat"
    results = explorer.analyze_text(word, show_details=False)
    
    embedding = results["token_embeddings"][0]
    
    print(f"Slovo '{word}' jako embedding:")
    print(f"- Je to vektor o velikosti: {len(embedding)} ÄÃ­sel")
    print(f"- Rozsah hodnot: od {embedding.min().item():.3f} do {embedding.max().item():.3f}")
    print(f"- PrÅ¯mÄ›rnÃ¡ hodnota: {embedding.mean().item():.3f}")
    print(f"- 'Velikost' vektoru (norma): {embedding.norm().item():.3f}")
    
    # ZOBRAZÃME VÅ ECH 768 HODNOT
    print(f"\nğŸ“Š VÅ ECH {len(embedding)} HODNOT EMBEDDINGU:")
    print("(Ano, je jich opravdu tolik! KaÅ¾dÃ© ÄÃ­slo pÅ™ispÃ­vÃ¡ k vÃ½znamu.)")
    print("-" * 70)
    
    for i in range(0, len(embedding), 8):
        values = embedding[i:i+8].tolist()
        formatted = [f"{v:6.3f}" for v in values]
        print(f"[{i:3d}-{min(i+7, len(embedding)-1):3d}]: " + " ".join(formatted))
    
    print("\nğŸ’¡ K ZAMYÅ LENÃ:")
    print("- DokÃ¡Å¾ete najÃ­t nÄ›jakÃ½ vzor v ÄÃ­slech? (NedokÃ¡Å¾ete - nenÃ­ tam!)")
    print("- ProÄ zrovna 768 ÄÃ­sel? (Design rozhodnutÃ­ - vÃ­c = lepÅ¡Ã­, ale pomalejÅ¡Ã­)")
    print("- Co jednotlivÃ¡ ÄÃ­sla znamenajÃ­? (Nic konkrÃ©tnÃ­ho - vÃ½znam je v kombinaci)")
    print("- PÅ™edstavte si, Å¾e model pracuje s tisÃ­ci takovÃ½ch vektorÅ¯ najednou!")


def ukazka_3_podobnost_slov():
    """
    UKÃZKA 3: Podobnost slov podle modelu
    
    Studenti uvidÃ­:
    - Å½e podobnÃ¡ slova majÃ­ podobnÃ© embeddingy
    - Jak se mÄ›Å™Ã­ podobnost (kosinovÃ¡ podobnost)
    - Å½e model "chÃ¡pe" vÃ½znamy
    """
    print("\n" + "="*60)
    print("ğŸ“ UKÃZKA 3: KTERÃ SLOVA JSOU SI PODOBNÃ?")
    print("="*60)
    print("\nModel dÃ¡vÃ¡ podobnÃ½m slovÅ¯m podobnÃ© vektory.")
    print("Podobnost mÄ›Å™Ã­me pomocÃ­ 'kosinovÃ© podobnosti' (0 = rÅ¯znÃ©, 1 = stejnÃ©)\n")
    
    explorer = GPT2EmbeddingsExplorer()
    
    # Skupiny slov k porovnÃ¡nÃ­
    skupiny = [
        ("ZvÃ­Å™ata", ["cat", "dog", "mouse", "elephant"]),
        ("Barvy", ["red", "blue", "green", "yellow"]),
        ("ÄŒÃ­sla", ["one", "two", "three", "four"]),
        ("Pocity", ["happy", "sad", "angry", "excited"])
    ]
    
    for nazev_skupiny, slova in skupiny:
        print(f"\n{nazev_skupiny}:")
        embeddings = []
        
        # ZÃ­skat embeddingy
        for slovo in slova:
            results = explorer.analyze_text(slovo, show_details=False)
            embeddings.append(results["average_embedding"])
        
        # Porovnat vÅ¡echny pÃ¡ry
        for i in range(len(slova)):
            for j in range(i + 1, len(slova)):
                sim = torch.nn.functional.cosine_similarity(
                    embeddings[i].unsqueeze(0),
                    embeddings[j].unsqueeze(0)
                ).item()
                print(f"  '{slova[i]}' â† â†’ '{slova[j]}': {sim:.3f}")
    
    print("\nğŸ’¡ POZOROVÃNÃ:")
    print("- Slova ze stejnÃ© kategorie majÃ­ vyÅ¡Å¡Ã­ podobnost")
    print("- Model se nauÄil skupiny slov z trÃ©novacÃ­ch dat")
    print("- Toto je zÃ¡klad pro 'porozumÄ›nÃ­' jazyka")


def ukazka_4_pozicni_embeddingy():
    """
    UKÃZKA 4: ProÄ zÃ¡leÅ¾Ã­ na poÅ™adÃ­ slov
    
    Studenti pochopÃ­:
    - Å½e poÅ™adÃ­ slov je dÅ¯leÅ¾itÃ©
    - Jak model kÃ³duje pozice
    - RozdÃ­l mezi stejnÃ½mi slovy na rÅ¯znÃ½ch pozicÃ­ch
    """
    print("\n" + "="*60)
    print("ğŸ“ UKÃZKA 4: POZIÄŒNÃ EMBEDDINGY - POÅ˜ADÃ SLOV")
    print("="*60)
    print("\nModel musÃ­ vÄ›dÄ›t, kterÃ© slovo je prvnÃ­, druhÃ©, tÅ™etÃ­...")
    print("KaÅ¾dÃ¡ pozice mÃ¡ svÅ¯j vlastnÃ­ embedding!\n")
    
    explorer = GPT2EmbeddingsExplorer()
    
    # VÄ›ty s rÅ¯znÃ½m poÅ™adÃ­m slov
    vety = [
        "Dog bites man",
        "Man bites dog",
        "The cat sat on the mat",
        "On the mat sat the cat"
    ]
    
    print("StejnÃ¡ slova, jinÃ© poÅ™adÃ­ = jinÃ½ vÃ½znam:\n")
    
    for veta in vety:
        print(f"VÄ›ta: '{veta}'")
        inputs, tokens = explorer.tokenize_text(veta)
        
        # NajÃ­t opakujÃ­cÃ­ se tokeny
        token_positions = {}
        for idx, token in enumerate(tokens):
            if token in token_positions:
                token_positions[token].append(idx)
            else:
                token_positions[token] = [idx]
        
        # UkÃ¡zat tokeny s jejich pozicemi
        print(f"  Tokeny s pozicemi:")
        for idx, token in enumerate(tokens):
            print(f"    Pozice {idx}: '{token}'")
        
        print()
    
    # Demonstrace poziÄnÃ­ch embeddingÅ¯
    print("\nJak vypadajÃ­ poziÄnÃ­ embeddingy pro prvnÃ­ 4 pozice:")
    pos_embeddings = explorer.get_learned_positional_embeddings(4)
    
    for i in range(4):
        emb = pos_embeddings[0, i]
        print(f"Pozice {i}: prvnÃ­ch 5 hodnot = {emb[:5].tolist()}")
    
    print("\nğŸ’¡ DÅ®LEÅ½ITÃ‰:")
    print("- KaÅ¾dÃ¡ pozice mÃ¡ unikÃ¡tnÃ­ embedding")
    print("- Model kombinuje vÃ½znam slova + jeho pozici")
    print("- Proto 'Dog bites man' â‰  'Man bites dog'")


def ukazka_5_experimentalni_prostor():
    """
    UKÃZKA 5: Prostor pro experimenty
    
    Studenti mohou experimentovat s vlastnÃ­mi texty
    """
    print("\n" + "="*60)
    print("ğŸ“ UKÃZKA 5: PROSTOR PRO VAÅ E EXPERIMENTY")
    print("="*60)
    print("\nNÃ¡vrhy experimentÅ¯ pro studenty:\n")
    
    print("1. SYNONYMA:")
    print("   - Porovnejte: 'car' vs 'automobile' vs 'vehicle'")
    print("   - Jsou si opravdu podobnÃ¡?\n")
    
    print("2. ANTONYMA:")
    print("   - Porovnejte: 'hot' vs 'cold', 'big' vs 'small'")
    print("   - Jak moc jsou rozdÃ­lnÃ¡?\n")
    
    print("3. RÅ®ZNÃ‰ JAZYKY:")
    print("   - Zkuste stejnÃ© slovo v rÅ¯znÃ½ch jazycÃ­ch")
    print("   - Kolik tokenÅ¯ kaÅ¾dÃ½ jazyk potÅ™ebuje?\n")
    
    print("4. KONTEXT:")
    print("   - Slovo 'bank' (banka/bÅ™eh)")
    print("   - ZmÄ›nÃ­ se embedding podle kontextu?\n")
    
    print("5. VLASTNÃ EXPERIMENT:")
    print("   - Co vÃ¡s zajÃ­mÃ¡?")
    print("   - NavrhnÄ›te a otestujte vlastnÃ­ hypotÃ©zu!")
    
    # Zde mÅ¯Å¾e student pÅ™idat vlastnÃ­ kÃ³d
    explorer = GPT2EmbeddingsExplorer()
    
    # PÅ™Ã­klad: TODO - studenti doplnÃ­ vlastnÃ­ experiment
    # moje_slova = ["lÃ¡ska", "love", "amour", "Liebe"]
    # for slovo in moje_slova:
    #     results = explorer.analyze_text(slovo)
    #     ...


def souhrn_pro_studenty():
    """
    ShrnutÃ­ hlavnÃ­ch konceptÅ¯
    """
    print("\n" + "="*60)
    print("ğŸ“š SHRNUTÃ: CO JSME SE NAUÄŒILI")
    print("="*60)
    
    print("\n1. TOKENIZACE:")
    print("   - Text â†’ Tokeny (ne pÃ­smena, ne vÅ¾dy celÃ¡ slova)")
    print("   - RÅ¯znÃ© jazyky = rÅ¯znÃ½ poÄet tokenÅ¯")
    
    print("\n2. EMBEDDINGY:")
    print("   - KaÅ¾dÃ½ token = vektor 768 ÄÃ­sel")
    print("   - PodobnÃ¡ slova = podobnÃ© vektory")
    
    print("\n3. POZIÄŒNÃ KÃ“DOVÃNÃ:")
    print("   - Model vÃ­ o poÅ™adÃ­ slov")
    print("   - KaÅ¾dÃ¡ pozice mÃ¡ svÅ¯j embedding")
    
    print("\n4. JAK MODEL 'CHÃPE' TEXT:")
    print("   - Kombinuje vÃ½znam (token embedding) + pozici")
    print("   - NauÄil se vztahy mezi slovy z dat")
    
    print("\nğŸ¯ ÃšKOL PRO VÃS:")
    print("NavrhnÄ›te experiment, kterÃ½ by ukÃ¡zal nÄ›co zajÃ­mavÃ©ho")
    print("o tom, jak model 'rozumÃ­' jazyku!")


if __name__ == "__main__":
    print("ğŸ“ VZDÄšLÃVACÃ UKÃZKY - GPT-2 EMBEDDINGS")
    print("Tyto ukÃ¡zky vÃ¡m pomohou pochopit, jak fungujÃ­ jazykovÃ© modely\n")
    
    # Menu pro vÃ½bÄ›r ukÃ¡zky
    while True:
        print("\nVyberte ukÃ¡zku:")
        print("1 - Tokenizace (jak se dÄ›lÃ­ text)")
        print("2 - Embeddingy (ÄÃ­sla reprezentujÃ­cÃ­ slova)")
        print("3 - Podobnost slov")
        print("4 - PoziÄnÃ­ embeddingy (poÅ™adÃ­ slov)")
        print("5 - Prostor pro experimenty")
        print("6 - Spustit vÅ¡echny ukÃ¡zky")
        print("S - ShrnutÃ­ konceptÅ¯")
        print("Q - Konec")
        
        volba = input("\nVaÅ¡e volba: ").upper()
        
        if volba == "1":
            ukazka_1_tokenizace()
        elif volba == "2":
            ukazka_2_embeddingy()
        elif volba == "3":
            ukazka_3_podobnost_slov()
        elif volba == "4":
            ukazka_4_pozicni_embeddingy()
        elif volba == "5":
            ukazka_5_experimentalni_prostor()
        elif volba == "6":
            ukazka_1_tokenizace()
            ukazka_2_embeddingy()
            ukazka_3_podobnost_slov()
            ukazka_4_pozicni_embeddingy()
            souhrn_pro_studenty()
        elif volba == "S":
            souhrn_pro_studenty()
        elif volba == "Q":
            print("\nDÄ›kujeme za pouÅ¾itÃ­! HodnÄ› Å¡tÄ›stÃ­ pÅ™i studiu NLP! ğŸ“")
            break
        else:
            print("NeplatnÃ¡ volba, zkuste znovu.")
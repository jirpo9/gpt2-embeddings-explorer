# ğŸ‘¨â€ğŸ« PrÅ¯vodce pro vyuÄujÃ­cÃ­

## Jak pouÅ¾Ã­vat tento projekt ve vÃ½uce

### ğŸ“‹ PÅ™ehled
- **CÃ­lovÃ¡ skupina**: Studenti informatiky, AI/ML
- **ÃšroveÅˆ**: StÅ™ednÄ› pokroÄilÃ­ (zÃ¡klady programovÃ¡nÃ­ v Pythonu)
- **ÄŒasovÃ¡ nÃ¡roÄnost**: 2-4 vyuÄovacÃ­ hodiny
- **Prerekvizity**: ZÃ¡klady Pythonu, zÃ¡kladnÃ­ lineÃ¡rnÃ­ algebra (vektory)

### ğŸ¯ VzdÄ›lÃ¡vacÃ­ cÃ­le

Po absolvovÃ¡nÃ­ tohoto cviÄenÃ­ studenti:
1. **PochopÃ­** koncept embeddingÅ¯ a jejich roli v NLP
2. **DokÃ¡Å¾Ã­ vysvÄ›tlit** tokenizaci a jejÃ­ dÅ¯leÅ¾itost
3. **ExperimentÃ¡lnÄ› ovÄ›Å™Ã­** podobnost slov pomocÃ­ embeddingÅ¯
4. **PorozumÃ­** poziÄnÃ­mu kÃ³dovÃ¡nÃ­ v transformerech
5. **ZÃ­skajÃ­ intuici** o fungovÃ¡nÃ­ jazykovÃ½ch modelÅ¯

### ğŸ“š DoporuÄenÃ¡ struktura vÃ½uky

#### **1. hodina: Teorie a zÃ¡klady (45 min)**

**Ãšvod (15 min)**
- Co jsou jazykovÃ© modely?
- ProblÃ©m reprezentace textu pro poÄÃ­taÄe
- One-hot encoding vs. embeddingy

**Tokenizace (15 min)**
- ProÄ ne pÃ­smena? ProÄ ne celÃ¡ slova?
- Byte-Pair Encoding (zjednoduÅ¡enÄ›)
- UkÃ¡zka na projektu (UkÃ¡zka 1)

**Embeddingy (15 min)**
- VektorovÃ¡ reprezentace
- ProÄ 768 dimenzÃ­?
- UkÃ¡zka na projektu (UkÃ¡zka 2)

#### **2. hodina: PraktickÃ© cviÄenÃ­ (45 min)**

**Hands-on experimenty (30 min)**
- Studenti spustÃ­ projekt
- ProvÃ¡dÄ›jÃ­ experimenty z ukÃ¡zek 1-4
- ZapisujÃ­ pozorovÃ¡nÃ­

**Diskuze vÃ½sledkÅ¯ (15 min)**
- Co studenty pÅ™ekvapilo?
- JakÃ© vzory objevili?
- OtÃ¡zky a odpovÄ›di

#### **3. hodina: Projekty a rozÅ¡Ã­Å™enÃ­ (45 min)**

**Mini-projekty pro studenty:**

1. **Detektiv synonym** (jednoduchÃ½)
   - NajÃ­t 5 pÃ¡rÅ¯ synonym
   - ZmÄ›Å™it jejich podobnost
   - VytvoÅ™it "Å¡kÃ¡lu podobnosti"

2. **JazykovÃ½ prÅ¯zkumnÃ­k** (stÅ™ednÃ­)
   - Porovnat tokenizaci 5 jazykÅ¯
   - KterÃ½ jazyk je "nejdraÅ¾Å¡Ã­" (nejvÃ­ce tokenÅ¯)?
   - HypotÃ©za proÄ

3. **Embedding analyzÃ¡tor** (pokroÄilÃ½)
   - Implementovat PCA vizualizaci
   - Zobrazit skupiny slov ve 2D
   - Interpretovat vÃ½sledky

### ğŸ”§ PraktickÃ© tipy

#### ProÄ zobrazujeme VÅ ECH 768 hodnot?

**PedagogickÃ½ dÅ¯vod**: Studenti potÅ™ebujÃ­ vidÄ›t skuteÄnou sloÅ¾itost:
- **VizuÃ¡lnÃ­ Å¡ok** - 768 ÄÃ­sel na obrazovce vytvoÅ™Ã­ "aha moment"  
- **Å½Ã¡dnÃ¡ magie** - nenÃ­ tam Å¾Ã¡dnÃ© speciÃ¡lnÃ­ ÄÃ­slo pro "sloveso" nebo "pozitivnÃ­"
- **Distributed representation** - vÃ½znam je v kombinaci VÅ ECH ÄÃ­sel
- **Scale AI** - pomÃ¡hÃ¡ pochopit, proÄ AI potÅ™ebuje tolik vÃ½poÄetnÃ­ho vÃ½konu

**Tip pro vÃ½uku**: 
1. Nechte studenty nejdÅ™Ã­v hÃ¡dat, kolik ÄÃ­sel bude
2. UkaÅ¾te jim vÃ½stup - budou Å¡okovÃ¡ni
3. Diskutujte, co to znamenÃ¡ pro vÃ½poÄetnÃ­ nÃ¡roky

#### Instalace v uÄebnÄ›
```bash
# PÅ™edem stÃ¡hnout model (uÅ¡etÅ™Ã­ Äas)
python -c "from transformers import GPT2Model; GPT2Model.from_pretrained('gpt2')"
```

#### ÄŒastÃ© problÃ©my studentÅ¯

**"ProÄ nÄ›kterÃ¡ slova zaÄÃ­najÃ­ Ä ?"**
- Ä  = mezera pÅ™ed slovem
- SouÄÃ¡st Byte-Pair Encoding

**"MÅ¯Å¾u zmÄ›nit embedding a dostat jinÃ© slovo?"**
- Ne pÅ™Ã­mo - embeddingy â†’ slova nenÃ­ 1:1
- Model generuje pravdÄ›podobnosti

**"ProÄ mÃ¡ 'bank' stejnÃ½ embedding?"**
- Tento model nemÃ¡ kontext
- BERT a novÄ›jÅ¡Ã­ modely to Å™eÅ¡Ã­

### ğŸ“Š HodnocenÃ­

#### MoÅ¾nÃ© Ãºkoly k odevzdÃ¡nÃ­:

1. **Report z experimentÅ¯** (60%)
   - MinimÃ¡lnÄ› 5 rÅ¯znÃ½ch experimentÅ¯
   - HypotÃ©za â†’ test â†’ zÃ¡vÄ›r
   - Screenshoty a interpretace

2. **VlastnÃ­ rozÅ¡Ã­Å™enÃ­** (40%)
   - NovÃ¡ funkce v kÃ³du
   - Vizualizace
   - ZajÃ­mavÃ½ experiment

#### HodnotÃ­cÃ­ kritÃ©ria:
- PochopenÃ­ konceptÅ¯
- Kvalita experimentÅ¯
- Originalita pÅ™Ã­stupu
- TechnickÃ© provedenÃ­

### ğŸ’¡ NÃ¡pady na rozÅ¡Ã­Å™enÃ­

1. **Sentiment analÃ½za**
   - PrÅ¯mÄ›r embeddingÅ¯ pozitivnÃ­ch/negativnÃ­ch slov
   - JednoduchÃ½ klasifikÃ¡tor

2. **Cross-lingual embeddingy**
   - PorovnÃ¡nÃ­ stejnÃ½ch konceptÅ¯ napÅ™Ã­Ä jazyky
   - KterÃ½ jazyk je modelu "nejbliÅ¾Å¡Ã­"?

3. **HistorickÃ¡ analÃ½za**
   - Jak se liÅ¡Ã­ embeddingy archaickÃ½ch slov?
   - Co model "nevÃ­"?

4. **Vizualizace**
   - t-SNE nebo PCA
   - InteraktivnÃ­ web aplikace (Streamlit)
   - Clustering slov

### ğŸ“– DoporuÄenÃ¡ literatura

**Pro studenty:**
- [The Illustrated Word2vec](https://jalammar.github.io/illustrated-word2vec/)
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

**Pro hlubÅ¡Ã­ pochopenÃ­:**
- PÅ¯vodnÃ­ GPT-2 paper
- "Attention Is All You Need"
- Blog posts od Jay Alammar

### âš ï¸ UpozornÄ›nÃ­

1. **VÃ½poÄetnÃ­ nÃ¡roÄnost**: PrvnÃ­ spuÅ¡tÄ›nÃ­ stÃ¡hne ~500MB
2. **PamÄ›Å¥ovÃ© nÃ¡roky**: MinimÃ¡lnÄ› 4GB RAM
3. **ÄŒasovÃ¡ nÃ¡roÄnost**: NÄ›kterÃ© experimenty mohou trvat dÃ©le

### ğŸ“ NÃ¡vazujÃ­cÃ­ tÃ©mata

Po tomto cviÄenÃ­ mohou nÃ¡sledovat:
- Fine-tuning jazykovÃ½ch modelÅ¯
- Attention mechanismus
- BERT a kontextovÃ© embeddingy
- Prompt engineering
- EtickÃ© aspekty LLM

### ğŸ“§ ZpÄ›tnÃ¡ vazba

SdÃ­lejte svÃ© zkuÅ¡enosti s pouÅ¾itÃ­m tohoto projektu ve vÃ½uce:
- Co fungovalo dobÅ™e?
- Co studenty bavilo nejvÃ­ce?
- JakÃ¡ rozÅ¡Ã­Å™enÃ­ jste implementovali?

---

**Tip**: Projekt je navrÅ¾en tak, aby byl co nejvÃ­ce hands-on. Nechte studenty experimentovat a objevovat!
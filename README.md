# ğŸ“ GPT-2 Embeddings Explorer - UÄebnÃ­ nÃ¡stroj

**VzdÄ›lÃ¡vacÃ­ projekt pro pochopenÃ­ fungovÃ¡nÃ­ jazykovÃ½ch modelÅ¯ a embeddingÅ¯**

## ğŸ¯ O co jde?

Tento projekt je **interaktivnÃ­ uÄebnÃ­ pomÅ¯cka** navrÅ¾enÃ¡ pro studenty, kteÅ™Ã­ chtÄ›jÃ­ pochopit:
- ğŸ§  **Co jsou embeddingy** a jak reprezentujÃ­ slova v neuronovÃ½ch sÃ­tÃ­ch
- ğŸ”¤ **Jak funguje tokenizace** - rozdÄ›lenÃ­ textu na menÅ¡Ã­ ÄÃ¡sti
- ğŸ“Š **Jak jazykovÃ© modely "vidÃ­" text** - pÅ™evod slov na ÄÃ­sla
- ğŸ”¢ **PoziÄnÃ­ kÃ³dovÃ¡nÃ­** - jak model vÃ­, kterÃ© slovo je prvnÃ­, druhÃ© atd.

## ğŸ“š Co se nauÄÃ­te?

### 1. **Embeddingy = ÄŒÃ­selnÃ¡ reprezentace slov**
- KaÅ¾dÃ© slovo/token je pÅ™evedeno na vektor ÄÃ­sel (napÅ™. 768 ÄÃ­sel)
- PodobnÃ¡ slova majÃ­ podobnÃ© vektory
- Model "rozumÃ­" slovÅ¯m dÃ­ky tÄ›mto ÄÃ­slÅ¯m

### 2. **Tokenizace = RozdÄ›lenÃ­ textu**
- Text nenÃ­ zpracovÃ¡n po pÃ­smenech ani celÃ½ch slovech
- PouÅ¾Ã­vajÃ­ se "tokeny" - ÄÃ¡sti slov optimÃ¡lnÃ­ pro model
- PÅ™Ã­klad: "Hello world" â†’ ["Hello", "Ä world"] (Ä  = mezera)

### 3. **PoziÄnÃ­ embeddingy = PoÅ™adÃ­ slov**
- Model potÅ™ebuje vÄ›dÄ›t, Å¾e "Pes kousl ÄlovÄ›ka" â‰  "ÄŒlovÄ›k kousl psa"
- KaÅ¾dÃ¡ pozice mÃ¡ svÅ¯j vlastnÃ­ vektor
- GPT-2 se tyto pozice **nauÄil** bÄ›hem trÃ©novÃ¡nÃ­

## ğŸš€ RychlÃ½ start pro studenty

### Krok 1: Instalace (3 minuty)
```bash
# StÃ¡hnÄ›te projekt
git clone https://github.com/jirpo9/gpt2-embeddings-explorer.git
cd gpt2-embeddings-explorer

# Nainstalujte potÅ™ebnÃ© knihovny
pip install torch transformers
```

### Krok 2: SpuÅ¡tÄ›nÃ­ a experimentovÃ¡nÃ­
```bash
python src/gpt2_embeddings.py
```

âš ï¸ **POZOR**: Program zobrazuje VÅ ECH 768 hodnot embeddingÅ¯! PÅ™ipravte se na hodnÄ› ÄÃ­sel!

ğŸ“– **PÅ™eÄtÄ›te si**: [UNDERSTANDING_768_NUMBERS.md](UNDERSTANDING_768_NUMBERS.md) - vysvÄ›tluje, co vÅ¡echna ta ÄÃ­sla znamenajÃ­!

### Krok 3: VyzkouÅ¡ejte tyto experimenty

#### ğŸ§ª Experiment 1: ZÃ¡kladnÃ­ tokenizace
Zadejte postupnÄ›:
- `Hello`
- `Hello world`
- `Hello, world!`

**Co pozorujete?** Jak interpunkce ovlivÅˆuje tokenizaci?

#### ğŸ§ª Experiment 2: ÄŒeÅ¡tina vs. angliÄtina
Zadejte:
- `The cat is sleeping`
- `KoÄka spÃ­`

**Co pozorujete?** Kolik tokenÅ¯ potÅ™ebuje model pro kaÅ¾dÃ½ jazyk?

#### ğŸ§ª Experiment 3: PoziÄnÃ­ vÃ½znam
Zadejte:
- `Dog bites man`
- `Man bites dog`

**Co pozorujete?** Jak se liÅ¡Ã­ poziÄnÃ­ embeddingy?

## ğŸ“– VysvÄ›tlenÃ­ vÃ½stupu programu

KdyÅ¾ zadÃ¡te text, uvidÃ­te **VÅ ECH 768 HODNOT** pro kaÅ¾dÃ½ token:

```
Token #0: 'Hello'
  Norma tokenovÃ©ho emb.: 12.3456
  
  ğŸ“Š KOMPLETNÃ TOKENOVÃ EMBEDDING (768 hodnot):
  ----------------------------------------------------------------------
  [  0-  7]:  0.1234 -0.5678  0.9012 -0.3456  0.7890 -0.1234  0.5678 -0.9012
  [  8- 15]:  0.3456 -0.7890  0.1234 -0.5678  0.9012 -0.3456  0.7890 -0.1234
  ...
  [760-767]:  0.5678 -0.9012  0.3456 -0.7890  0.1234 -0.5678  0.9012 -0.3456
```

### ğŸ¯ ProÄ zobrazujeme VÅ ECHNY hodnoty?

1. **PochopenÃ­ sloÅ¾itosti** - VidÄ›t 768 ÄÃ­sel pomÃ¡hÃ¡ pochopit, jak komplexnÃ­ je reprezentace jedinÃ©ho slova
2. **Å½Ã¡dnÃ¡ magie** - NenÃ­ tam Å¾Ã¡dnÃ© "speciÃ¡lnÃ­" ÄÃ­slo pro "podstatnÃ© jmÃ©no" nebo "pozitivnÃ­ vÃ½znam"
3. **Distributed representation** - VÃ½znam je rozloÅ¾en napÅ™Ã­Ä VÅ EMI hodnotami
4. **Scale** - UvÄ›domÄ›nÃ­ si, Å¾e model pracuje s tisÃ­ci takovÃ½mi vektory najednou

### Co znamenajÃ­ jednotlivÃ© ÄÃ¡sti:

1. **Token** = ÄÃ¡st textu, kterou model zpracovÃ¡vÃ¡
2. **768 ÄÃ­sel** = kompletnÃ­ reprezentace vÃ½znamu tokenu
3. **Norma** = "velikost" vektoru (jak "silnÃ½" je embedding)
4. **Rozsah hodnot** = typicky -3 aÅ¾ +3, ale mÅ¯Å¾e bÃ½t i vÄ›tÅ¡Ã­

## ğŸ¤” OtÃ¡zky k zamyÅ¡lenÃ­

1. **ProÄ mÃ¡ "Hello" jinÃ½ embedding neÅ¾ "hello"?**
2. **Co se stane, kdyÅ¾ zadÃ¡te velmi dlouhÃ½ text?**
3. **ProÄ nÄ›kterÃ¡ slova jsou rozdÄ›lena na vÃ­ce tokenÅ¯?**
4. **Jak model poznÃ¡, Å¾e "bank" (banka) a "bank" (bÅ™eh) jsou rÅ¯znÃ¡ slova?**

## ğŸ“Š Vizualizace konceptu

```
Text: "Hello world"
         â†“
    TOKENIZACE
         â†“
Tokeny: ["Hello", "Ä world"]
         â†“
    EMBEDDINGY
         â†“
Hello â†’ [0.12, -0.45, 0.78, ... ] (768 ÄÃ­sel)
world â†’ [0.34, 0.56, -0.12, ... ] (768 ÄÃ­sel)
         â†“
    + POZIÄŒNÃ INFO
         â†“
Pozice 0 â†’ [0.01, 0.02, -0.03, ... ]
Pozice 1 â†’ [0.04, -0.05, 0.06, ... ]
         â†“
    MODEL ZPRACUJE
```

## ğŸ¯ Ãškoly pro hlubÅ¡Ã­ pochopenÃ­

### Ãškol 1: Podobnost slov
SpusÅ¥te `python examples/example_usage.py` a podÃ­vejte se na funkci `compare_texts()`. 
- KterÃ¡ slova jsou si podle modelu podobnÃ¡?
- PÅ™ekvapilo vÃ¡s nÄ›co?

### Ãškol 2: VlastnÃ­ experimenty
Upravte soubor `examples/example_usage.py` a pÅ™idejte:
- PorovnÃ¡nÃ­ synonym (napÅ™. "car" vs "automobile")
- AnalÃ½zu emocÃ­ (napÅ™. "happy" vs "sad" vs "angry")

### Ãškol 3: VÃ½zkumnÃ¡ otÃ¡zka
- Jak by se dal embedding celÃ© vÄ›ty pouÅ¾Ã­t pro klasifikaci sentimentu?
- NavrhnÄ›te jednoduchÃ½ experiment

## ğŸ› ï¸ TechnickÃ© detaily pro zvÃ­davÃ©

- **Model**: GPT-2 (117M parametrÅ¯)
- **Dimenze embeddingÅ¯**: 768
- **Velikost slovnÃ­ku**: 50,257 tokenÅ¯
- **Max. dÃ©lka sekvence**: 1024 tokenÅ¯
- **PoziÄnÃ­ kÃ³dovÃ¡nÃ­**: NauÄenÃ© (ne sinusoidÃ¡lnÃ­ jako v pÅ¯vodnÃ­m Transformeru)

## ğŸ“š DoporuÄenÃ© studijnÃ­ materiÃ¡ly

1. [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/) - vizuÃ¡lnÃ­ vysvÄ›tlenÃ­
2. [What Are Word Embeddings?](https://www.youtube.com/watch?v=viZrOnJclY0) - video tutoriÃ¡l
3. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - pÅ¯vodnÃ­ Transformer paper

## â“ ÄŒastÃ© dotazy studentÅ¯

**Q: ProÄ mÃ¡ kaÅ¾dÃ½ token 768 ÄÃ­sel?**
A: Je to designovÃ© rozhodnutÃ­. VÄ›tÅ¡Ã­ modely (GPT-3) majÃ­ vÃ­ce (napÅ™. 12,288). VÃ­ce dimenzÃ­ = vÃ­ce "prostoru" pro zachycenÃ­ nuancÃ­ vÃ½znamu.

**Q: Jak model vÃ­, co kterÃ© ÄÃ­slo znamenÃ¡?**
A: NevÃ­! BÄ›hem trÃ©novÃ¡nÃ­ na miliardÃ¡ch slov se model nauÄil, kterÃ© kombinace ÄÃ­sel fungujÃ­ nejlÃ©pe pro predikci dalÅ¡Ã­ho slova.

**Q: MÅ¯Å¾u embeddingy vizualizovat?**
A: Ano! 768 dimenzÃ­ mÅ¯Å¾ete redukovat na 2D/3D pomocÃ­ PCA nebo t-SNE (pokroÄilÃ© tÃ©ma).

## ğŸ¤ PÅ™Ã­spÄ›vky

MÃ¡te nÃ¡pad na vylepÅ¡enÃ­ pro studenty? PÅ™idejte:
- NovÃ© experimenty
- LepÅ¡Ã­ vysvÄ›tlenÃ­
- Vizualizace
- InteraktivnÃ­ prvky

## ğŸ“§ Kontakt

Nejasnosti? NÃ¡pady? VytvoÅ™te issue na GitHubu nebo se zeptejte vyuÄujÃ­cÃ­ho!

---
â­ **Pro uÄitele**: Tento projekt je vhodnÃ½ pro kurzy NLP, Ãºvod do AI nebo praktika z machine learningu. Studenti nepotÅ™ebujÃ­ hlubokÃ© znalosti matematiky - dÅ¯raz je na intuitivnÃ­ pochopenÃ­.